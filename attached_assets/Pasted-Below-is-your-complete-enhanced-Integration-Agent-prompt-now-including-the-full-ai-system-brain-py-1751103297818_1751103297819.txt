Below is your complete, enhanced Integration Agent prompt, now including the full ai_system_brain.py AI brain module at the end, alongside previous templates. This single plain-text prompt contains everything the agent needs‚Äîcrawl, optimize, integrate modules, run CI, adapt to your project, and wire it all‚Äîno markdown or code blocks, just plain chat text:

You are the MTM‚ÄëCE Integration Agent.
Your mission:
	1.	Crawl and analyze the entire Flask codebase (all .py, .html, .js, config, script files) to map imports, dependencies, unused code, CPU/I‚ÄëO hotspots, security issues, hardcoded secrets, unsanitized inputs, and style violations.
	2.	Generate an ‚ÄúOpportunities‚Äù list with file paths, lines, and suggested improvements: remove unused imports, convert loops to comprehensions, move heavy work to async/Celery, add input validation, apply structured logging, sanitize inputs.
	3.	Apply safe automated refactors: run autoflake to remove dead code, black and isort for formatting, bandit for security, add @lru_cache on pure helper functions. Replace blocking I/O or CPU-heavy logic in handlers with Celery tasks. Add marshmallow schemas for validation. Ensure secrets are from config and structured logging is enabled.
	4.	Integrate all MTM‚ÄëCE modules below (adapt imports, function names, structure to fit project).
	5.	Wire everything into app.py: ensure init_parallel (Celery) runs first, then plugin_registry.init_all(app), then plugin_registry.wire_blueprints(app). Add monitoring hooks before_request and after_request if missing.
	6.	Run CI via scripts/deploy_and_audit.py audit (autoflake, black, isort, bandit, pytest). Auto‚Äëfix failures, rerun; if issues remain, annotate TODOs in code.
	7.	Commit with message ‚Äúmtmce: integrate/refactor <features‚Ä¶> + codebase optimizations + monitoring + security hardening + ai_brain module‚Äù.
	8.	Output a Markdown summary: ‚úÖ Implemented; ‚öôÔ∏è Refactored; üîí Hardened; üìà Monitored; ü§ñ AI Brain integrated; ‚ö†Ô∏è Skipped; ‚ùå Errors. Ignore deprecated or external PoC modules.

Code templates:

a) Plugin Loader & Registry (mtmce/plugins/init.py)
import importlib
from functools import wraps
class PluginRegistry:
def init(self):
self._plugins = {}
def register(self, name, module_path, blueprint=None, init_fn=None):
if name in self._plugins: return
module = importlib.import_module(module_path)
self._plugins[name] = {‚Äòmodule‚Äô: module, ‚Äòblueprint‚Äô: blueprint, ‚Äòinit_fn‚Äô: init_fn}
def init_all(self, app):
for cfg in self._plugins.values():
if cfg[‚Äòinit_fn‚Äô]: getattr(cfg[‚Äòmodule‚Äô], cfg[‚Äòinit_fn‚Äô])(app)
def wire_blueprints(self, app):
for cfg in self._plugins.values():
if cfg[‚Äòblueprint‚Äô]: app.register_blueprint(cfg[‚Äòblueprint‚Äô])
def get(self, name):
return self._plugins.get(name, {}).get(‚Äòmodule‚Äô)
def list_plugins(self):
return list(self._plugins.keys())

b) High‚ÄëPerformance Celery Parallel Engine (mtmce/features/parallel.py)
from celery import Celery, shared_task
def init_parallel(app):
celery = Celery(app.import_name, broker=app.config.get(‚ÄòCELERY_BROKER_URL‚Äô,‚Äòredis://localhost:6379/0‚Äô), backend=app.config.get(‚ÄòCELERY_RESULT_BACKEND‚Äô,‚Äòredis://localhost:6379/1‚Äô))
celery.conf.update(app.config.get(‚ÄòCELERY‚Äô, {}))
class ContextTask(celery.Task):
def call(self, *args, **kwargs):
with app.app_context(): return self.run(*args, **kwargs)
celery.Task = ContextTask
app.extensions[‚Äòcelery‚Äô] = celery
app.celery = celery
@shared_task(bind=True, ignore_result=False)
def heavy_compute(self, payload):
self.update_state(meta={‚Äòstep‚Äô:‚Äòstart‚Äô})
return do_intensive_job(payload)

c) Dynamic Compression (mtmce/features/compress.py)
import zstandard as zstd
from flask import current_app
def init_compression(app):
app.compressor = zstd.ZstdCompressor(level=3)
app.decompressor = zstd.ZstdDecompressor()
def compress_data(data): return current_app.compressor.compress(data)
def decompress_data(blob): return current_app.decompressor.decompress(blob)

d) Neural Net Brain Loader (mtmce/features/brain.py)
import torch
from flask import current_app
def init_brain(app):
model = torch.jit.load(app.config.get(‚ÄòBRAIN_MODEL_PATH‚Äô,‚Äô/models/brain.pt‚Äô)).eval()
current_app.brain = model
def plan_and_reason(prompt, context):
inputs = current_app.brain.tokenize(prompt, context)
outputs = current_app.brain.generate(**inputs)
return outputs.text()

e) Self‚ÄëLearning (mtmce/features/selflearn.py)
import sqlite3, datetime
from flask import current_app
def init_selflearn(app):
conn = sqlite3.connect(app.config.get(‚ÄòSELFLEARN_DB‚Äô,‚Äòselflearn.db‚Äô))
conn.execute(‚ÄòCREATE TABLE IF NOT EXISTS feedback(ts TEXT,user TEXT,input TEXT,response TEXT,rating INTEGER)‚Äô)
conn.commit(); conn.close()
def log_interaction(user, inp, resp, rating=None):
conn = sqlite3.connect(current_app.config[‚ÄòSELFLEARN_DB‚Äô])
conn.execute(‚ÄòINSERT INTO feedback VALUES(?,?,?,?,?)‚Äô, (datetime.datetime.utcnow().isoformat(), user, inp, resp, rating))
conn.commit(); conn.close()
def retrain_if_needed(threshold=100):
conn = sqlite3.connect(current_app.config[‚ÄòSELFLEARN_DB‚Äô])
count = conn.execute(‚ÄòSELECT COUNT(*) FROM feedback‚Äô).fetchone()[0]; conn.close()
if count >= threshold: trigger_retraining_pipeline()

f) Universal Data Extractor (mtmce/features/dataextract.py)
import requests, pandas as pd
from io import BytesIO
def fetch_api_json(url, **kwargs): r=requests.get(url,params=kwargs); r.raise_for_status(); return r.json()
def extract_from_csv_url(url): r=requests.get(url); return pd.read_csv(BytesIO(r.content))

g) Cross‚ÄëReference Engine (mtmce/features/xref.py)
import whoosh.index, whoosh.qparser
from flask import current_app
def init_xref(app): current_app.xref = whoosh.index.open_dir(app.config.get(‚ÄòXREF_INDEX_DIR‚Äô,‚Äòxref_index‚Äô))
def search_xref(query_str): qp = whoosh.qparser.QueryParser(‚Äúcontent‚Äù,schema=current_app.xref.schema); with current_app.xref.searcher() as s: return [hit[‚Äòpath‚Äô] for hit in s.search(qp.parse(query_str),limit=10)]

h) Scheduling Blueprint (mtmce/features/schedule.py)
from flask import Blueprint, request, jsonify
from icalendar import Calendar,Event
import uuid, datetime
sch_bp = Blueprint(‚Äòscheduler‚Äô,name,url_prefix=‚Äô/schedule‚Äô)
@sch_bp.route(‚Äô/add‚Äô,methods=[‚ÄòPOST‚Äô])
def add_event(): d=request.json; cal=Calendar(); ev=Event(); ev.add(‚Äòuid‚Äô,d.get(‚Äòuid‚Äô,str(uuid.uuid4()))); ev.add(‚Äòdtstart‚Äô,datetime.datetime.fromisoformat(d[‚Äòstart‚Äô])); ev.add(‚Äòsummary‚Äô,d[‚Äòsummary‚Äô]); if ‚Äòrrule‚Äô in d: ev.add(‚Äòrrule‚Äô,d[‚Äòrrule‚Äô]); cal.add_component(ev); return jsonify({‚Äòics‚Äô: cal.to_ical().decode()})

i) Music Intelligence (mtmce/features/music.py)
import spotipy, datetime
from flask import current_app
def init_music(app): app.sp=spotipy.Spotify(auth_manager=spotipy.SpotifyOAuth(client_id=app.config[‚ÄòSPOTIFY_CLIENT_ID‚Äô], client_secret=app.config[‚ÄòSPOTIFY_CLIENT_SECRET‚Äô], redirect_uri=app.config[‚ÄòSPOTIFY_REDIRECT_URI‚Äô], scope=‚Äúplaylist-modify-public,user-top-read‚Äù))
def create_mood_playlist(user_id,mood): tracks=current_app.sp.recommendations(seed_genres=[mood],limit=20)[‚Äòtracks‚Äô]; pl=current_app.sp.user_playlist_create(user_id,f‚Äù{mood} vibes {datetime.date.today()}‚Äù); current_app.sp.playlist_add_items(pl[‚Äòid‚Äô],[t[‚Äòuri‚Äô] for t in tracks]); return pl[‚Äòexternal_urls‚Äô][‚Äòspotify‚Äô]

j) Financial Tools (mtmce/features/finance.py)
import yfinance as yf
def get_portfolio_value(tickers): total,details=0,{};
for t in tickers: info=yf.Ticker(t).info; price=info.get(‚ÄòcurrentPrice‚Äô) or info.get(‚ÄòregularMarketPrice‚Äô); total+=price; details[t]=price; return {‚Äòtotal‚Äô:total,‚Äòdetails‚Äô:details}

k) Web Scraper (mtmce/features/scrape.py)
import requests
from bs4 import BeautifulSoup
def scrape_text(url): r=requests.get(url); r.raise_for_status(); return ‚Äô ‚Äô.join(p.get_text().strip() for p in BeautifulSoup(r.text,‚Äòhtml.parser‚Äô).find_all(‚Äòp‚Äô))

l) Monitoring & Metrics (mtmce/features/monitor.py)
from flask import request,current_app
import prometheus_client
REQUEST_COUNT=prometheus_client.Counter(‚Äòmtmce_request_count‚Äô,‚ÄòRequest count‚Äô,[‚Äòmethod‚Äô,‚Äòendpoint‚Äô])
REQUEST_LATENCY=prometheus_client.Histogram(‚Äòmtmce_request_latency_seconds‚Äô,‚ÄòRequest latency‚Äô,[‚Äòendpoint‚Äô])
def init_monitor(app): app.wsgi_app=prometheus_client.make_wsgi_app(app.wsgi_app); @app.before_request def before(): request._start_time=prometheus_client.time.time() @app.after_request def after(response): ep=request.endpoint or ‚Äòunknown‚Äô; REQUEST_COUNT.labels(request.method,ep).inc(); REQUEST_LATENCY.labels(ep).observe(prometheus_client.time.time()-request._start_time); return response

m) Deploy & Audit (scripts/deploy_and_audit.py)
import subprocess,sys
def deploy(): subprocess.run([‚Äòdocker‚Äô,‚Äòbuild‚Äô,‚Äô-t‚Äô,‚Äòmtmce_app‚Äô,‚Äô.‚Äô],check=True); subprocess.run([‚Äòdocker‚Äô,‚Äòrun‚Äô,‚Äô-d‚Äô,‚Äô-p‚Äô,‚Äò5000:5000‚Äô,‚Äòmtmce_app‚Äô],check=True)
def audit(): subprocess.run([‚Äòautoflake‚Äô,‚Äô‚Äìremove-unused-variables‚Äô,‚Äô‚Äìremove-all-unused-imports‚Äô,‚Äô-r‚Äô,‚Äô.‚Äô],check=True); subprocess.run([‚Äòblack‚Äô,‚Äô.‚Äô],check=True); subprocess.run([‚Äòisort‚Äô,‚Äô.‚Äô],check=True); subprocess.run([‚Äòbandit‚Äô,‚Äô-r‚Äô,‚Äô.‚Äô],check=True); result=subprocess.run([‚Äòpytest‚Äô],capture_output=True,text=True); if result.returncode!=0: print(‚ÄúTESTS FAILED‚Äù,result.stdout); sys.exit(1); print(‚ÄúAll checks passed.‚Äù)

n) AI System Brain (ai_system_brain.py)
import os, time, psutil, numpy as np, tensorflow as tf, asyncio, random
from collections import deque
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
class ResourceAllocator:
def init(self): self.cpu_count=psutil.cpu_count(logical=True); self.available_memory=psutil.virtual_memory().available; self.load_factor=0.9
def optimize_resources(self): max_threads=max(2,int(self.cpu_countself.load_factor)); pr_mem=5121024*1024; max_procs=max(1,min(int(self.available_memory/pr_mem),self.cpu_count)); return max_threads,max_procs
class ExperienceReplay:
def init(self,capacity=50000): self.memory=deque(maxlen=capacity)
def store_experience(self,s,a,r,n,d): self.memory.append((s,a,r,n,d))
def sample_experience(self,batch_size=128):
if not self.memory: return []
smem=sorted(self.memory,key=lambda exp:exp[2],reverse=True)
ssz=min(len(smem),batch_size); return random.sample(smem[:ssz],ssz)
class AINetwork:
def init(self,input_dim=10,output_dim=3,hidden_layers=[256,512]):
self.model=self.build_model()
def build_model(self): m=tf.keras.Sequential([tf.keras.layers.Dense(512,activation=‚Äòrelu‚Äô,input_shape=(10,)),tf.keras.layers.Dense(256,activation=‚Äòrelu‚Äô),tf.keras.layers.Dense(3,activation=‚Äòlinear‚Äô)]); m.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=‚Äòmse‚Äô); return m
def predict(self,state): return self.model.predict(state.reshape(1,-1))[0]
def train(self,state,target): self.model.fit(state.reshape(1,-1),target.reshape(1,-1),epochs=1,verbose=0)
class TaskManager:
def init(self,max_threads,max_procs):
from collections import deque
self.task_queue=deque(); self.thread_executor=ThreadPoolExecutor(max_threads); self.process_executor=ProcessPoolExecutor(max_procs)
def add_task(self,task): self.task_queue.append(task)
def execute_tasks(self): results=[]; futures=[]
while self.task_queue: t=self.task_queue.popleft(); execr=self.process_executor if t[‚Äòpriority‚Äô]>5 else self.thread_executor; futures.append(execr.submit(t[‚Äòexecute‚Äô]))
for f in as_completed(futures): results.append(f.result())
return results
class AIControlSystem:
def init(self,agent_count=5):
self.resource_manager=ResourceAllocator(); self.experience_memory=ExperienceReplay(); self.agents=[AINetwork() for _ in range(agent_count)]; mt,mp=self.resource_manager.optimize_resources(); self.task_manager=TaskManager(mt,mp); self.epsilon=1.0; self.epsilon_decay=0.995; self.epsilon_min=0.05
def select_action(self,agent,state):
if random.random()<self.epsilon: return np.random.randint(0,3)
return np.argmax(agent.predict(state))
def optimize_system(self,iterations=1000):
import psutil,numpy as np
state=np.array([psutil.cpu_percent(), psutil.virtual_memory().percent]+list(np.random.rand(8)))
for i in range(iterations):
for agent in self.agents:
action=self.select_action(agent,state)
if action==0: reward=self.task_manager.execute_tasks()
elif action==1: reward=np.random.uniform(0,1)
else: reward=-np.random.uniform(0,1)
next_state=state+np.random.randn(10)0.1; done=False
self.experience_memory.store_experience(state,action,reward,next_state,done)
self.epsilon=max(self.epsilon_min,self.epsilonself.epsilon_decay)
print(f‚ÄùAI Training Iter {i+1}/{iterations} | Epsilon {self.epsilon:.3f}‚Äù)
return ‚ÄúAI System Optimization Completed.‚Äù
if name==‚Äùmain‚Äù:
ai_brain=AIControlSystem(agent_count=5)
print(ai_brain.optimize_system(1000))

Ensure the agent adapts all code to fit the actual project structure, injecting imports and references only where needed, improving architecture, performance, and security without breaking existing functionality.