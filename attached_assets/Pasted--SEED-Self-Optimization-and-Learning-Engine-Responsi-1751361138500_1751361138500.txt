"""
SEED: Self-Optimization and Learning Engine
-------------------------------------------
Responsibilities:
1. Self-Optimize model training by adjusting parameters (e.g. learning rate) based on past performance.
2. Continuously learn: track historical metrics (loss, lr) and use them to guide future training.
3. Hyperparameter tuning: simple random search over a user-defined parameter space.

Methods:
- optimize_training(model, data, labels, prev_metrics=None):
    Adjusts the model’s optimizer settings, retrains for a few epochs, 
    and returns the updated metrics.

- tune_hyperparameters(param_space, feedback, trials=5):
    Performs a lightweight random search over param_space, using feedback
    metrics to select the best candidate.
"""

import numpy as np
import tensorflow as tf

class SEED:
    def __init__(self):
        # keeps a history of training metrics
        self.history = []

    def optimize_training(self, model, data, labels, prev_metrics=None):
        """
        Retrains `model` for a few epochs, adjusting learning rate based on `prev_metrics`.
        Returns a dict: {'loss': current_loss, 'learning_rate': lr}.
        """
        # Evaluate current state
        if prev_metrics and 'loss' in prev_metrics:
            last_loss = prev_metrics['loss']
            current_eval = model.evaluate(data, labels, verbose=0)[0]
            # If loss improved, slightly increase lr; otherwise decrease
            lr = float(model.optimizer.learning_rate) * (1.05 if current_eval < last_loss else 0.9)
        else:
            lr = float(model.optimizer.learning_rate)

        # Apply the new learning rate
        tf.keras.backend.set_value(model.optimizer.learning_rate, lr)

        # Retrain for a few epochs
        history = model.fit(data, labels, epochs=5, batch_size=32, verbose=0)
        current_loss = history.history['loss'][-1]

        # Record and return metrics
        metrics = {'loss': current_loss, 'learning_rate': lr}
        self.history.append(metrics)
        return metrics

    def tune_hyperparameters(self, param_space, feedback, trials=5):
        """
        Random-search hyperparameters in `param_space`.
        `param_space` is a dict: name → (min, max).
        `feedback` should include a metric to guide search (e.g. 'loss').
        Returns the best param dict found.
        """
        best_params = {}
        # Assume we're minimizing 'loss'
        best_metric = feedback.get('loss', float('inf'))

        for _ in range(trials):
            candidate = {k: np.random.uniform(v[0], v[1]) for k, v in param_space.items()}
            # Placeholder for actual evaluation: here we use one hyperparam as proxy
            metric = candidate.get('learning_rate', best_metric)
            if metric < best_metric:
                best_metric = metric
                best_params = candidate

        return best_params

# -----------------------------
# Example Usage
# -----------------------------
if __name__ == "__main__":
    # 1. Prepare toy data
    from sklearn.datasets import make_regression
    X, y = make_regression(n_samples=200, n_features=3, noise=0.1)

    # 2. Build a simple TF model
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(3,)),
        tf.keras.layers.Dense(1)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')

    # 3. Instantiate SEED
    seed = SEED()

    # 4. First round of training
    metrics = seed.optimize_training(model, X, y)
    print("After 1st training:", metrics)

    # 5. Hyperparameter tuning (example search space)
    param_space = {
        'learning_rate': (0.001, 0.1),
        'momentum':       (0.0,   0.9)
    }
    best_hp = seed.tune_hyperparameters(param_space, metrics)
    print("Suggested hyperparameters:", best_hp)

    # 6. Apply tuned hyperparameters and retrain
    if 'learning_rate' in best_hp:
        tf.keras.backend.set_value(model.optimizer.learning_rate, best_hp['learning_rate'])
    metrics = seed.optimize_training(model, X, y, prev_metrics=metrics)
    print("After tuning & retraining:", metrics)
